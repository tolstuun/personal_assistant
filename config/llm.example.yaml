# LLM Configuration
# Copy this file to llm.yaml and fill in your API keys

default_provider: litellm
default_model: claude-sonnet-4-20250514

providers:
  litellm:
    temperature: 0.7
    max_tokens: 4096
    timeout: 60.0
    extra: {}

aliases:
  fast: gpt-4o-mini
  smart: claude-sonnet-4-20250514
  smartest: claude-opus-4-20250514
  local: ollama/llama3

task_models:
  summarization: gpt-4o-mini
  extraction: claude-sonnet-4-20250514
  analysis: claude-sonnet-4-20250514
  translation: gpt-4o-mini
  code: claude-sonnet-4-20250514
